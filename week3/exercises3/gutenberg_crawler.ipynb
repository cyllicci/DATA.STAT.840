{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\EmmaK\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\EmmaK\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EmmaK\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create a directory for NLTK data if it doesn't exist\n",
    "nltk_data_dir = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "    os.makedirs(nltk_data_dir)\n",
    "\n",
    "# Append this directory to NLTK data path\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download the resources again\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "\n",
    "Gutenberg_home_page_url = 'https://www.gutenberg.org'\n",
    "Gutenberg_top_page_url = Gutenberg_home_page_url + '/browse/scores/top'\n",
    "\n",
    "if not os.path.exists(\"../week2/downloaded_books\"):\n",
    "    os.makedirs(\"../week2/downloaded_books\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def getpagetext(parsedpage):\n",
    "    scriptelements=parsedpage.find_all('script')\n",
    "    for scriptelement in scriptelements:\n",
    "        scriptelement.extract()\n",
    "    pagetext=parsedpage.get_text()\n",
    "    return pagetext\n",
    "\n",
    "def parse_webpage(url):\n",
    "    try:\n",
    "        response=requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        parsed_html=bs4.BeautifulSoup(response.content,'html.parser')\n",
    "        return parsed_html\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetcing {url}: {e}\" )\n",
    "        return None\n",
    "\n",
    "def extract_actual_book_content(read_online_url, book_title):\n",
    "    parsed_html = parse_webpage(read_online_url)\n",
    "    if parsed_html is None:\n",
    "        return \"\"\n",
    "    full_text = getpagetext(parsed_html)\n",
    "    HEADER_TEXT_MARKER = \"*** START OF THE PROJECT GUTENBERG EBOOK \" + book_title.upper() + \" ***\"\n",
    "    FOOTER_TEXT_MARKER = \"*** END OF THE PROJECT GUTENBERG EBOOK \" + book_title.upper() + \" ***\"\n",
    "    start_index = full_text.find(HEADER_TEXT_MARKER) + len(HEADER_TEXT_MARKER)\n",
    "    end_index = full_text.find(FOOTER_TEXT_MARKER)\n",
    "    return full_text[start_index:end_index].strip()\n",
    "\n",
    "def save_text_to_file(author, title, content):\n",
    "    clean_title = ''.join(c for c in title if c.isalnum() or c.isspace()).replace(' ', '_')\n",
    "    file_path = os.path.join(\"../week2/downloaded_books\", f\"{clean_title}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "        print(f\"saved {title} by {author} to {file_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error saving {title} by {author} to {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books that will be downloaded:\n",
      "1. Author: Shelley, Mary Wollstonecraft, 1797-1851, Title: Frankenstein; Or, The Modern Prometheus\n",
      "2. Author: Shakespeare, William, 1564-1616, Title: Romeo and Juliet\n",
      "3. Author: Austen, Jane, 1775-1817, Title: Pride and Prejudice\n",
      "4. Author: Melville, Herman, 1819-1891, Title: Moby Dick; Or, The Whale\n",
      "5. Author: Eliot, George, 1819-1880, Title: Middlemarch\n",
      "6. Author: Shakespeare, William, 1564-1616, Title: The Complete Works of William Shakespeare\n",
      "7. Author: Forster, E. M. (Edward Morgan), 1879-1970, Title: A Room with a View\n",
      "8. Author: Carroll, Lewis, 1832-1898, Title: Alice's Adventures in Wonderland\n",
      "9. Author: Alcott, Louisa May, 1832-1888, Title: Little Women; Or, Meg, Jo, Beth, and Amy\n",
      "10. Author: Von Arnim, Elizabeth, 1866-1941, Title: The Enchanted April\n",
      "11. Author: Montgomery, L. M. (Lucy Maud), 1874-1942, Title: The Blue Castle: a novel\n",
      "12. Author: Hawthorne, Nathaniel, 1804-1864, Title: The Scarlet Letter\n",
      "13. Author: Smollett, T. (Tobias), 1721-1771, Title: The Adventures of Ferdinand Count Fathom — Complete\n",
      "14. Author: Gaskell, Elizabeth Cleghorn, 1810-1865, Title: Cranford\n",
      "15. Author: Smollett, T. (Tobias), 1721-1771, Title: The Expedition of Humphry Clinker\n",
      "16. Author: Fielding, Henry, 1707-1754, Title: History of Tom Jones, a Foundling\n",
      "17. Author: Smollett, T. (Tobias), 1721-1771, Title: The Adventures of Roderick Random\n",
      "18. Author: Wagner, Richard, 1813-1883, Title: My Life — Volume 1\n",
      "19. Author: Dumas, Alexandre, 1802-1870, Title: Twenty years after\n",
      "20. Author: Ibsen, Henrik, 1828-1906, Title: A Doll's House : a play\n",
      "\n",
      "Downloading book 1/20: Frankenstein; Or, The Modern Prometheus by Shelley, Mary Wollstonecraft, 1797-1851\n",
      "saved Frankenstein; Or, The Modern Prometheus by Shelley, Mary Wollstonecraft, 1797-1851 to downloaded_books\\Frankenstein_Or_The_Modern_Prometheus\n",
      "Downloading book 2/20: Romeo and Juliet by Shakespeare, William, 1564-1616\n",
      "saved Romeo and Juliet by Shakespeare, William, 1564-1616 to downloaded_books\\Romeo_and_Juliet\n",
      "Downloading book 3/20: Pride and Prejudice by Austen, Jane, 1775-1817\n",
      "saved Pride and Prejudice by Austen, Jane, 1775-1817 to downloaded_books\\Pride_and_Prejudice\n",
      "Downloading book 4/20: Moby Dick; Or, The Whale by Melville, Herman, 1819-1891\n",
      "saved Moby Dick; Or, The Whale by Melville, Herman, 1819-1891 to downloaded_books\\Moby_Dick_Or_The_Whale\n",
      "Downloading book 5/20: Middlemarch by Eliot, George, 1819-1880\n",
      "saved Middlemarch by Eliot, George, 1819-1880 to downloaded_books\\Middlemarch\n",
      "Downloading book 6/20: The Complete Works of William Shakespeare by Shakespeare, William, 1564-1616\n",
      "saved The Complete Works of William Shakespeare by Shakespeare, William, 1564-1616 to downloaded_books\\The_Complete_Works_of_William_Shakespeare\n",
      "Downloading book 7/20: A Room with a View by Forster, E. M. (Edward Morgan), 1879-1970\n",
      "saved A Room with a View by Forster, E. M. (Edward Morgan), 1879-1970 to downloaded_books\\A_Room_with_a_View\n",
      "Downloading book 8/20: Alice's Adventures in Wonderland by Carroll, Lewis, 1832-1898\n",
      "saved Alice's Adventures in Wonderland by Carroll, Lewis, 1832-1898 to downloaded_books\\Alices_Adventures_in_Wonderland\n",
      "Downloading book 9/20: Little Women; Or, Meg, Jo, Beth, and Amy by Alcott, Louisa May, 1832-1888\n",
      "saved Little Women; Or, Meg, Jo, Beth, and Amy by Alcott, Louisa May, 1832-1888 to downloaded_books\\Little_Women_Or_Meg_Jo_Beth_and_Amy\n",
      "Downloading book 10/20: The Enchanted April by Von Arnim, Elizabeth, 1866-1941\n",
      "saved The Enchanted April by Von Arnim, Elizabeth, 1866-1941 to downloaded_books\\The_Enchanted_April\n",
      "Downloading book 11/20: The Blue Castle: a novel by Montgomery, L. M. (Lucy Maud), 1874-1942\n",
      "saved The Blue Castle: a novel by Montgomery, L. M. (Lucy Maud), 1874-1942 to downloaded_books\\The_Blue_Castle_a_novel\n",
      "Downloading book 12/20: The Scarlet Letter by Hawthorne, Nathaniel, 1804-1864\n",
      "saved The Scarlet Letter by Hawthorne, Nathaniel, 1804-1864 to downloaded_books\\The_Scarlet_Letter\n",
      "Downloading book 13/20: The Adventures of Ferdinand Count Fathom — Complete by Smollett, T. (Tobias), 1721-1771\n",
      "saved The Adventures of Ferdinand Count Fathom — Complete by Smollett, T. (Tobias), 1721-1771 to downloaded_books\\The_Adventures_of_Ferdinand_Count_Fathom__Complete\n",
      "Downloading book 14/20: Cranford by Gaskell, Elizabeth Cleghorn, 1810-1865\n",
      "saved Cranford by Gaskell, Elizabeth Cleghorn, 1810-1865 to downloaded_books\\Cranford\n",
      "Downloading book 15/20: The Expedition of Humphry Clinker by Smollett, T. (Tobias), 1721-1771\n",
      "saved The Expedition of Humphry Clinker by Smollett, T. (Tobias), 1721-1771 to downloaded_books\\The_Expedition_of_Humphry_Clinker\n",
      "Downloading book 16/20: History of Tom Jones, a Foundling by Fielding, Henry, 1707-1754\n",
      "saved History of Tom Jones, a Foundling by Fielding, Henry, 1707-1754 to downloaded_books\\History_of_Tom_Jones_a_Foundling\n",
      "Downloading book 17/20: The Adventures of Roderick Random by Smollett, T. (Tobias), 1721-1771\n",
      "saved The Adventures of Roderick Random by Smollett, T. (Tobias), 1721-1771 to downloaded_books\\The_Adventures_of_Roderick_Random\n",
      "Downloading book 18/20: My Life — Volume 1 by Wagner, Richard, 1813-1883\n",
      "saved My Life — Volume 1 by Wagner, Richard, 1813-1883 to downloaded_books\\My_Life__Volume_1\n",
      "Downloading book 19/20: Twenty years after by Dumas, Alexandre, 1802-1870\n",
      "saved Twenty years after by Dumas, Alexandre, 1802-1870 to downloaded_books\\Twenty_years_after\n",
      "Downloading book 20/20: A Doll's House : a play by Ibsen, Henrik, 1828-1906\n",
      "saved A Doll's House : a play by Ibsen, Henrik, 1828-1906 to downloaded_books\\A_Dolls_House__a_play\n"
     ]
    }
   ],
   "source": [
    "def crawl_ebook(relative_link, author, title):\n",
    "    e_book_url = Gutenberg_home_page_url + relative_link\n",
    "    parsed_html = parse_webpage(e_book_url)\n",
    "\n",
    "    read_online_link = parsed_html.find('a', title='Read online')['href']\n",
    "    actual_content = extract_actual_book_content(Gutenberg_home_page_url + read_online_link, title)\n",
    "    save_text_to_file(author, title, actual_content)\n",
    "\n",
    "def get_author_and_title(relative_link):\n",
    "    e_book_url = Gutenberg_home_page_url + relative_link\n",
    "    parsed_html = parse_webpage(e_book_url)\n",
    "\n",
    "    if parsed_html is None:\n",
    "        return \"Unknown Author\", \"Unknown Title\"\n",
    "\n",
    "    try:\n",
    "        book_author = parsed_html.find('a', itemprop=\"creator\").text.strip()\n",
    "        book_title = parsed_html.find('td', itemprop=\"headline\").text.strip()\n",
    "    except AttributeError:\n",
    "        print(f\"Error parsing author or title for {relative_link}\")\n",
    "        return \"Unknown Author\", \"Unknown Title\"\n",
    "    \n",
    "    return book_author, book_title\n",
    "\n",
    "def print_book_list(books):\n",
    "    for idx, (author, title) in enumerate(books, 1):\n",
    "        print(f\"{idx}. Author: {author}, Title: {title}\")\n",
    "\n",
    "def gutenberg_top_k_ebook_crawler(top_page_url, k):\n",
    "    parsed_html = parse_webpage(top_page_url)\n",
    "\n",
    "    books_last_30_header = parsed_html.find('h2', id='books-last30')\n",
    "    book_list = books_last_30_header.find_next('ol')\n",
    "    book_items = book_list.find_all('li')\n",
    "    e_book_links = []\n",
    "    for book_item in book_items[0:k]:\n",
    "        a_element = book_item.find_next('a')\n",
    "        e_book_links.append(a_element['href'])\n",
    "\n",
    "    books = [get_author_and_title(link) for link in e_book_links]\n",
    "    \n",
    "    print(\"Books that will be downloaded:\")\n",
    "    print_book_list(books)\n",
    "\n",
    "    print()\n",
    "\n",
    "    for i, (link, (author, title)) in enumerate(zip(e_book_links, books)):\n",
    "        print(f\"Downloading book {i+1}/{k}: {title} by {author}\")\n",
    "        crawl_ebook(link, author, title)\n",
    "\n",
    "gutenberg_top_k_ebook_crawler(Gutenberg_top_page_url, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 words in the downloaded books:\n",
      "the: 153898\n",
      "of: 97619\n",
      "to: 96664\n",
      "and: 96274\n",
      "a: 94100\n",
      "I: 70335\n",
      "in: 54990\n",
      "that: 42800\n",
      "wa: 36177\n",
      "his: 35919\n",
      "he: 34156\n",
      "with: 32679\n",
      "you: 32406\n",
      "it: 31904\n",
      "my: 30270\n",
      "her: 29121\n",
      "not: 28712\n",
      "for: 28057\n",
      "had: 26241\n",
      "is: 24019\n",
      "be: 23400\n",
      "me: 21275\n",
      "s: 20157\n",
      "have: 20037\n",
      "him: 19227\n",
      "at: 19165\n",
      "this: 18923\n",
      "she: 18916\n",
      "which: 18088\n",
      "on: 16878\n",
      "by: 16797\n",
      "but: 16316\n",
      "all: 14619\n",
      "so: 14607\n",
      "from: 12902\n",
      "your: 12298\n",
      "The: 12162\n",
      "And: 11931\n",
      "will: 11124\n",
      "would: 11090\n",
      "said: 10780\n",
      "no: 10226\n",
      "an: 9861\n",
      "were: 9823\n",
      "one: 9811\n",
      "who: 9775\n",
      "are: 9752\n",
      "they: 9257\n",
      "we: 9116\n",
      "if: 8682\n",
      "do: 8533\n",
      "or: 8249\n",
      "been: 8176\n",
      "what: 8159\n",
      "more: 8153\n",
      "their: 7830\n",
      "when: 7642\n",
      "But: 7625\n",
      "them: 7582\n",
      "very: 7158\n",
      "than: 6933\n",
      "there: 6838\n",
      "now: 6834\n",
      "He: 6833\n",
      "could: 6778\n",
      "our: 6728\n",
      "out: 6434\n",
      "man: 6425\n",
      "some: 6362\n",
      "time: 6244\n",
      "up: 6125\n",
      "such: 6078\n",
      "upon: 6056\n",
      "shall: 6053\n",
      "good: 6029\n",
      "should: 5965\n",
      "d: 5914\n",
      "did: 5904\n",
      "can: 5873\n",
      "know: 5835\n",
      "thou: 5796\n",
      "any: 5687\n",
      "am: 5659\n",
      "like: 5647\n",
      "into: 5636\n",
      "then: 5528\n",
      "much: 5282\n",
      "say: 5190\n",
      "It: 5075\n",
      "must: 4988\n",
      "little: 4941\n",
      "To: 4900\n",
      "come: 4853\n",
      "t: 4787\n",
      "make: 4770\n",
      "other: 4728\n",
      "never: 4613\n",
      "see: 4613\n",
      "well: 4609\n",
      "u: 4576\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]  # Ignore punctuation\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def process_books(directory=\"../week2/downloaded_books\"):\n",
    "    unified_vocabulary = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                lemmatized_tokens = tokenize_and_lemmatize(text)\n",
    "                for token in lemmatized_tokens:\n",
    "                    unified_vocabulary[token] = unified_vocabulary.get(token, 0) + 1\n",
    "                #pruned_unified_vocabulary = prune(unified_vocabulary)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: {filename} not found in {directory}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "    #return pruned_unified_vocabulary\n",
    "    return sorted(unified_vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Process the books and create a unified vocabulary\n",
    "vocabulary = process_books()\n",
    "\n",
    "# Sort and display the top 100 words by frequency\n",
    "top_100_words = vocabulary[:100]\n",
    "print(\"Top 100 words in the downloaded books:\")\n",
    "for word, freq in top_100_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 153898\n",
      "of 97619\n",
      "to 96664\n",
      "and 96274\n",
      "a 94100\n",
      "I 70335\n",
      "in 54990\n",
      "that 42800\n",
      "wa 36177\n",
      "his 35919\n",
      "he 34156\n",
      "with 32679\n",
      "you 32406\n",
      "it 31904\n",
      "my 30270\n",
      "her 29121\n",
      "not 28712\n",
      "for 28057\n",
      "had 26241\n",
      "is 24019\n",
      "be 23400\n",
      "me 21275\n",
      "s 20157\n",
      "have 20037\n",
      "him 19227\n",
      "at 19165\n",
      "this 18923\n",
      "she 18916\n",
      "which 18088\n",
      "on 16878\n",
      "by 16797\n",
      "but 16316\n",
      "all 14619\n",
      "so 14607\n",
      "from 12902\n",
      "your 12298\n",
      "The 12162\n",
      "And 11931\n",
      "will 11124\n",
      "would 11090\n",
      "said 10780\n",
      "no 10226\n",
      "an 9861\n",
      "were 9823\n",
      "one 9811\n",
      "who 9775\n",
      "are 9752\n",
      "they 9257\n",
      "we 9116\n",
      "if 8682\n",
      "do 8533\n",
      "or 8249\n",
      "been 8176\n",
      "what 8159\n",
      "more 8153\n",
      "their 7830\n",
      "when 7642\n",
      "But 7625\n",
      "them 7582\n",
      "very 7158\n",
      "than 6933\n",
      "there 6838\n",
      "now 6834\n",
      "He 6833\n",
      "could 6778\n",
      "our 6728\n",
      "out 6434\n",
      "man 6425\n",
      "some 6362\n",
      "time 6244\n",
      "up 6125\n",
      "such 6078\n",
      "upon 6056\n",
      "shall 6053\n",
      "good 6029\n",
      "should 5965\n",
      "d 5914\n",
      "did 5904\n",
      "can 5873\n",
      "know 5835\n",
      "thou 5796\n",
      "any 5687\n",
      "am 5659\n",
      "like 5647\n",
      "into 5636\n",
      "then 5528\n",
      "much 5282\n",
      "say 5190\n",
      "It 5075\n",
      "must 4988\n",
      "little 4941\n",
      "To 4900\n",
      "come 4853\n",
      "t 4787\n",
      "make 4770\n",
      "other 4728\n",
      "never 4613\n",
      "see 4613\n",
      "well 4609\n",
      "u 4576\n",
      "What 4570\n",
      "That 4565\n",
      "own 4527\n",
      "thy 4477\n",
      "made 4433\n",
      "only 4404\n",
      "about 4287\n",
      "may 4286\n",
      "She 4282\n",
      "great 4255\n",
      "You 4222\n",
      "day 4181\n",
      "before 4175\n",
      "A 4159\n",
      "love 4124\n",
      "go 4118\n",
      "these 4063\n",
      "hand 4043\n",
      "think 3959\n",
      "thee 3950\n",
      "most 3925\n",
      "being 3908\n",
      "himself 3878\n",
      "too 3792\n",
      "thought 3730\n",
      "might 3655\n",
      "here 3643\n",
      "eye 3630\n",
      "friend 3620\n",
      "ha 3613\n",
      "life 3583\n",
      "In 3403\n",
      "thing 3327\n",
      "ll 3325\n",
      "This 3303\n",
      "old 3296\n",
      "two 3285\n",
      "where 3271\n",
      "after 3263\n",
      "how 3244\n",
      "take 3243\n",
      "My 3237\n",
      "first 3232\n",
      "sir 3185\n",
      "without 3158\n",
      "way 3153\n",
      "yet 3118\n",
      "If 3102\n",
      "lord 3052\n",
      "heart 3045\n",
      "give 3040\n",
      "again 3010\n",
      "down 2982\n",
      "For 2982\n",
      "As 2973\n",
      "young 2946\n",
      "O 2919\n",
      "those 2917\n",
      "word 2896\n",
      "let 2860\n",
      "though 2842\n",
      "over 2761\n",
      "ever 2711\n",
      "lady 2696\n",
      "father 2685\n",
      "away 2648\n",
      "look 2639\n",
      "every 2615\n",
      "hath 2610\n",
      "house 2573\n",
      "long 2546\n",
      "No 2527\n",
      "tell 2526\n",
      "nothing 2495\n",
      "men 2491\n",
      "part 2489\n",
      "many 2488\n",
      "even 2467\n",
      "same 2447\n",
      "whom 2402\n",
      "head 2400\n",
      "last 2397\n",
      "Enter 2390\n",
      "myself 2381\n",
      "came 2373\n",
      "still 2346\n",
      "place 2318\n",
      "world 2218\n",
      "woman 2218\n",
      "poor 2209\n",
      "We 2200\n",
      "found 2197\n",
      "gentleman 2190\n",
      "mind 2189\n",
      "There 2156\n",
      "mean 2153\n",
      "Miss 2140\n",
      "put 2136\n",
      "When 2136\n",
      "How 2132\n",
      "while 2132\n",
      "off 2125\n",
      "face 2102\n",
      "having 2096\n",
      "dear 2093\n",
      "mother 2093\n",
      "seemed 2068\n",
      "better 2065\n",
      "night 2056\n",
      "once 2053\n",
      "another 2052\n",
      "name 2040\n",
      "Why 2012\n",
      "went 1997\n",
      "So 1980\n",
      "find 1976\n",
      "herself 1957\n",
      "against 1945\n",
      "back 1928\n",
      "They 1922\n",
      "through 1911\n",
      "speak 1893\n",
      "done 1880\n",
      "Mr 1878\n",
      "always 1876\n",
      "year 1867\n",
      "both 1853\n",
      "saw 1846\n",
      "hope 1842\n",
      "indeed 1837\n",
      "people 1801\n",
      "heard 1796\n",
      "letter 1766\n",
      "nor 1750\n",
      "told 1750\n",
      "just 1743\n",
      "took 1739\n",
      "whose 1738\n",
      "hear 1696\n",
      "soon 1688\n",
      "Then 1671\n",
      "room 1667\n",
      "Jones 1654\n",
      "leave 1653\n",
      "death 1650\n",
      "under 1649\n",
      "Artagnan 1645\n",
      "matter 1642\n",
      "left 1639\n",
      "rather 1637\n",
      "Now 1636\n",
      "D 1616\n",
      "whole 1608\n",
      "person 1597\n",
      "child 1594\n",
      "mine 1585\n",
      "King 1582\n",
      "seen 1581\n",
      "true 1580\n",
      "wife 1576\n",
      "Well 1575\n",
      "set 1565\n",
      "till 1564\n",
      "Sir 1561\n",
      "get 1560\n",
      "With 1558\n",
      "home 1557\n",
      "honour 1554\n",
      "going 1547\n",
      "hour 1544\n",
      "kind 1542\n",
      "felt 1535\n",
      "God 1535\n",
      "looked 1534\n",
      "therefore 1529\n",
      "work 1524\n",
      "enough 1521\n",
      "gave 1520\n",
      "right 1518\n",
      "three 1515\n",
      "present 1503\n",
      "something 1495\n",
      "asked 1490\n",
      "sure 1487\n",
      "best 1486\n",
      "quite 1469\n",
      "moment 1452\n",
      "knew 1452\n",
      "son 1448\n",
      "wish 1443\n",
      "art 1435\n",
      "door 1434\n",
      "Let 1433\n",
      "far 1432\n",
      "cried 1429\n",
      "His 1428\n",
      "began 1426\n",
      "want 1422\n",
      "Of 1422\n",
      "call 1420\n",
      "fear 1417\n",
      "brother 1411\n",
      "Come 1409\n",
      "since 1404\n",
      "reason 1399\n",
      "At 1397\n",
      "Yes 1390\n",
      "between 1390\n",
      "king 1389\n",
      "Jo 1389\n",
      "gone 1387\n",
      "manner 1384\n",
      "keep 1382\n",
      "new 1374\n",
      "KING 1372\n",
      "end 1370\n",
      "got 1365\n",
      "soul 1351\n",
      "arm 1344\n",
      "Here 1340\n",
      "side 1339\n",
      "husband 1339\n",
      "cry 1334\n",
      "le 1330\n",
      "Is 1327\n",
      "By 1325\n",
      "thus 1324\n",
      "fellow 1321\n",
      "daughter 1296\n",
      "because 1288\n",
      "called 1287\n",
      "almost 1285\n",
      "anything 1284\n",
      "full 1274\n",
      "fair 1273\n",
      "master 1269\n",
      "sister 1262\n",
      "order 1262\n",
      "believe 1257\n",
      "voice 1256\n",
      "Do 1244\n",
      "st 1244\n",
      "de 1239\n",
      "help 1238\n",
      "morning 1232\n",
      "Thou 1227\n",
      "power 1215\n",
      "often 1209\n",
      "however 1208\n",
      "family 1208\n",
      "few 1204\n",
      "spirit 1200\n",
      "answered 1195\n",
      "each 1191\n",
      "Mrs 1179\n",
      "bear 1175\n",
      "least 1168\n",
      "Will 1158\n",
      "next 1152\n",
      "fortune 1150\n",
      "Oh 1147\n",
      "brought 1146\n",
      "among 1145\n",
      "turn 1144\n",
      "light 1143\n",
      "nature 1142\n",
      "taken 1135\n",
      "company 1135\n",
      "towards 1132\n",
      "o 1131\n",
      "blood 1129\n",
      "perhaps 1127\n",
      "money 1125\n",
      "heaven 1125\n",
      "course 1124\n",
      "Exeunt 1120\n",
      "live 1116\n",
      "Which 1114\n",
      "use 1113\n",
      "girl 1112\n",
      "boy 1109\n",
      "answer 1104\n",
      "Who 1104\n",
      "certain 1102\n",
      "doth 1102\n",
      "given 1099\n",
      "stand 1094\n",
      "don 1090\n",
      "care 1086\n",
      "together 1083\n",
      "Lord 1076\n",
      "dead 1075\n",
      "Exit 1063\n",
      "also 1057\n",
      "horse 1054\n",
      "turned 1051\n",
      "alone 1051\n",
      "else 1048\n",
      "rest 1048\n",
      "air 1047\n",
      "sweet 1043\n",
      "already 1037\n",
      "Your 1037\n",
      "happy 1034\n",
      "really 1032\n",
      "Not 1031\n",
      "feeling 1028\n",
      "replied 1023\n",
      "whale 1022\n",
      "show 1020\n",
      "Athos 1020\n",
      "sea 1011\n",
      "half 1009\n",
      "doe 1003\n",
      "bed 996\n",
      "return 995\n",
      "th 994\n",
      "Valancy 994\n",
      "Lydgate 992\n",
      "country 991\n",
      "desire 988\n",
      "within 987\n",
      "truth 981\n",
      "received 977\n",
      "hold 974\n",
      "tear 974\n",
      "round 973\n",
      "occasion 970\n",
      "sort 969\n",
      "noble 967\n",
      "near 960\n",
      "read 948\n",
      "Where 948\n",
      "lie 947\n",
      "All 941\n",
      "need 940\n",
      "ti 939\n",
      "subject 938\n",
      "Dorothea 936\n",
      "business 932\n",
      "CHAPTER 931\n",
      "why 928\n",
      "On 927\n",
      "play 926\n",
      "looking 926\n",
      "cause 923\n",
      "small 920\n",
      "lay 912\n",
      "Sophia 912\n",
      "pleasure 910\n",
      "Or 909\n",
      "lost 907\n",
      "thousand 900\n",
      "foot 895\n",
      "point 891\n",
      "others 888\n",
      "feel 887\n",
      "town 887\n",
      "case 886\n",
      "queen 886\n",
      "question 885\n",
      "stay 883\n",
      "known 882\n",
      "sent 880\n",
      "character 876\n",
      "SCENE 874\n",
      "bring 870\n",
      "body 867\n",
      "state 865\n",
      "short 862\n",
      "opinion 862\n",
      "Lady 861\n",
      "itself 859\n",
      "uncle 853\n",
      "ye 852\n",
      "servant 850\n",
      "Ay 849\n",
      "ship 846\n",
      "saying 842\n",
      "account 842\n",
      "themselves 842\n",
      "purpose 841\n",
      "open 837\n",
      "none 835\n",
      "evening 832\n",
      "Porthos 832\n",
      "fire 829\n",
      "idea 823\n",
      "doubt 823\n",
      "Tis 822\n",
      "yourself 821\n",
      "whether 818\n",
      "Her 816\n",
      "immediately 814\n",
      "forth 810\n",
      "stood 808\n",
      "sight 807\n",
      "ear 807\n",
      "FIRST 805\n",
      "understand 800\n",
      "making 796\n",
      "Good 796\n",
      "meet 794\n",
      "die 794\n",
      "sword 794\n",
      "everything 792\n",
      "either 790\n",
      "effect 787\n",
      "book 786\n",
      "fall 786\n",
      "used 786\n",
      "fact 785\n",
      "strange 777\n",
      "high 775\n",
      "water 775\n",
      "talk 775\n",
      "returned 772\n",
      "Chapter 769\n",
      "Scene 769\n",
      "pray 764\n",
      "Elizabeth 763\n",
      "tongue 762\n",
      "coming 759\n",
      "minute 759\n",
      "able 757\n",
      "ask 756\n",
      "longer 755\n",
      "sat 754\n",
      "particular 754\n",
      "beauty 750\n",
      "Yet 749\n",
      "remember 740\n",
      "Go 738\n",
      "ready 733\n",
      "Duke 732\n",
      "Nay 732\n",
      "change 730\n",
      "hard 727\n",
      "general 726\n",
      "er 725\n",
      "kept 721\n",
      "pretty 717\n",
      "become 716\n",
      "sense 712\n",
      "joy 712\n"
     ]
    }
   ],
   "source": [
    "def prune(vocab):\n",
    "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    pruned_vocab = []\n",
    "    for word, freq in vocab:\n",
    "        # rule 1: check the nltk stop words list\n",
    "        if(word in nltk_stop_words):\n",
    "            continue\n",
    "        # rule 2: check if is in the top 1% of frequent words\n",
    "        if(freq >= vocab[int(len(vocab)/100)][1]):\n",
    "            continue\n",
    "        # rule 3: if the word occurs less than 4 times\n",
    "        if(freq < 4):\n",
    "            continue\n",
    "        # rule 4: word is overly short (less than 3 characters) or long (over than 15 characters)\n",
    "        if(len(freq) < 3 or len(freq) > 15):\n",
    "            continue\n",
    "        else:\n",
    "            pruned_vocab.append((word,freq))\n",
    "    return pruned_vocab\n",
    "\n",
    "pruned_vocabulary = prune(vocabulary)\n",
    "print(pruned_vocabulary[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataml100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
